# Asgard Data Platform

A comprehensive FastAPI-based data platform that provides unified REST APIs for end-to-end data operations including data ingestion via Airbyte and data transformations using Kubernetes SparkOperator.

## üèóÔ∏è Architecture Overview

The Asgard Data Platform follows a modular, cloud-native architecture with clear separation of concerns:

```mermaid
graph TB
    subgraph "Client Layer"
        API[REST API Clients]
        WEB[Web Applications]
        CLI[CLI Tools]
    end

    subgraph "API Gateway Layer"
        FASTAPI[FastAPI Gateway<br/>Port 8000]
    end

    subgraph "Service Layer"
        AIRBYTE_SVC[Airbyte Service]
        TRANSFORM_SVC[Transformation Service]
    end

    subgraph "Client Layer - Internal"
        AIRBYTE_CLIENT[Airbyte Client]
        SPARK_CLIENT[Spark Client]
    end

    subgraph "External Systems"
        AIRBYTE[Airbyte Server<br/>Port 8001]
        K8S[Kubernetes Cluster<br/>SparkOperator]
    end

    subgraph "Data Storage"
        S3[S3 Compatible Storage<br/>Bronze/Silver/Gold Layers]
    end

    API --> FASTAPI
    WEB --> FASTAPI
    CLI --> FASTAPI

    FASTAPI --> AIRBYTE_SVC
    FASTAPI --> TRANSFORM_SVC

    AIRBYTE_SVC --> AIRBYTE_CLIENT
    TRANSFORM_SVC --> SPARK_CLIENT
    TRANSFORM_SVC --> AIRBYTE_CLIENT

    AIRBYTE_CLIENT --> AIRBYTE
    SPARK_CLIENT --> K8S

    AIRBYTE --> S3
    K8S --> S3
```

## üîÑ Data Flow Architecture

### Complete Data Pipeline Flow

```mermaid
flowchart TD
    subgraph "Data Sources"
        PG[PostgreSQL]
        MYSQL[MySQL]
        MONGO[MongoDB]
        KAFKA[Kafka]
        API_SRC[API Sources]
    end

    subgraph "Ingestion Layer"
        AIRBYTE[Airbyte<br/>Data Integration]
    end

    subgraph "Bronze Layer (Raw Data)"
        S3_BRONZE[S3 Bronze<br/>Raw Data Storage]
    end

    subgraph "Processing Layer"
        SPARK[Spark on Kubernetes<br/>Data Transformation]
    end

    subgraph "Silver Layer (Processed Data)"
        S3_SILVER[S3 Silver<br/>Cleaned & Transformed]
    end

    subgraph "Gold Layer (Analytics Ready)"
        S3_GOLD[S3 Gold<br/>Business Aggregates]
    end

    subgraph "Consumption"
        BI[BI Tools]
        ML[ML Pipelines]
        ANALYTICS[Analytics Apps]
    end

    PG --> AIRBYTE
    MYSQL --> AIRBYTE
    MONGO --> AIRBYTE
    KAFKA --> AIRBYTE
    API_SRC --> AIRBYTE

    AIRBYTE --> S3_BRONZE
    S3_BRONZE --> SPARK
    SPARK --> S3_SILVER
    S3_SILVER --> SPARK
    SPARK --> S3_GOLD

    S3_SILVER --> BI
    S3_GOLD --> BI
    S3_GOLD --> ML
    S3_GOLD --> ANALYTICS
```

### API Request Flow

```mermaid
sequenceDiagram
    participant Client
    participant Router
    participant Service
    participant K8sClient
    participant AirbyteClient
    participant Kubernetes
    participant Airbyte
    participant S3

    Note over Client,S3: Data Transformation Flow

    Client->>Router: POST /transform
    Router->>Service: submit_transformation()
    Service->>AirbyteClient: get S3 destinations
    AirbyteClient->>Airbyte: list destinations
    Airbyte-->>AirbyteClient: S3 configs
    AirbyteClient-->>Service: S3 paths
    Service->>K8sClient: create_spark_application()
    K8sClient->>Kubernetes: Create SparkApplication
    Kubernetes-->>K8sClient: Application created
    K8sClient-->>Service: Job submitted
    Service-->>Router: Job details
    Router-->>Client: Job status & run_id

    Note over Kubernetes,S3: Background Processing
    Kubernetes->>S3: Read bronze data
    Kubernetes->>S3: Write silver data

    Note over Client,Kubernetes: Monitoring Flow
    Client->>Router: GET /transform/{run_id}/status
    Router->>Service: get_job_status()
    Service->>K8sClient: get_spark_application()
    K8sClient->>Kubernetes: Get SparkApplication
    Kubernetes-->>K8sClient: Status details
    K8sClient-->>Service: Job status
    Service-->>Router: Status info
    Router-->>Client: Current status
```

## üöÄ Quick Start

### Prerequisites

- Python 3.11+
- [uv](https://docs.astral.sh/uv/) package manager
- Running Airbyte instance (for data ingestion)
- Kubernetes cluster with SparkOperator (for transformations)
- S3-compatible storage

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd asgard-dev

# Install dependencies
uv sync

# Copy environment configuration
cp .env.example .env
```

### 2. Configuration

Edit `.env` with your service configuration:

```bash
# Airbyte Configuration
AIRBYTE_BASE_URL=http://localhost:8001/api/public/v1

# Kubernetes Configuration
SPARK_IMAGE=your-registry/spark-custom:latest
SPARK_SERVICE_ACCOUNT=spark-sa
S3_SECRET_NAME=s3-credentials

# Environment
ENVIRONMENT=development
```

### 3. Start the Platform

```bash
# Start the API server
uv run python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The API will be available at:

- **API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

## üìñ API Reference

### Data Ingestion APIs (Airbyte Integration)

#### List Data Sources

```bash
GET /datasource
```

#### Create Data Source

```bash
POST /datasource
Content-Type: application/json

{
  "source_type": "postgres",
  "workspace_name": "default",
  "source_config": {
    "host": "localhost",
    "port": 5432,
    "username": "user",
    "password": "password",
    "database": "mydb"
  },
  "name": "My Postgres Source"
}
```

#### List Data Sinks

```bash
GET /sink
```

#### Create Data Sink

```bash
POST /sink
Content-Type: application/json

{
  "destination_type": "s3",
  "workspace_name": "default",
  "destination_config": {
    "s3_bucket_name": "my-data-bucket",
    "s3_bucket_path": "bronze/",
    "aws_access_key_id": "your_key",
    "aws_secret_access_key": "your_secret"
  },
  "name": "My S3 Sink"
}
```

#### Start Data Ingestion

```bash
POST /ingestion
Content-Type: application/json

{
  "source_id": "source-uuid-here",
  "destination_id": "destination-uuid-here",
  "workspace_name": "default",
  "connection_name": "Postgres to S3 Sync"
}
```

### Data Transformation APIs (Spark Integration)

#### Submit Transformation Job

```bash
POST /transform
Content-Type: application/json

# Minimal request (recommended)
{
  "sql": "SELECT customer_id, SUM(amount) as total_amount FROM source_data GROUP BY customer_id"
}

# With custom Spark configuration
{
  "sql": "SELECT customer_id, SUM(amount) as total_amount FROM source_data GROUP BY customer_id",
  "write_mode": "overwrite",
  "executor_instances": 4,
  "executor_cores": 2,
  "executor_memory": "8g",
  "driver_cores": 2,
  "driver_memory": "4g"
}
```

#### Get Job Status

```bash
GET /transform/{run_id}/status
```

Response:

```json
{
  "run_id": "107ea972",
  "spark_application": "sql-exec-107ea972",
  "namespace": "default",
  "state": "COMPLETED",
  "driver_pod": "sql-exec-107ea972-driver",
  "executor_count": 2,
  "creation_time": "2025-08-27T10:04:59Z",
  "spark_version": "3.5.1"
}
```

#### Get Job Logs

```bash
GET /transform/{run_id}/logs
```

#### Get Job Events

```bash
GET /transform/{run_id}/events
```

#### Get Job Metrics

```bash
GET /transform/{run_id}/metrics
```

#### List All Jobs

```bash
GET /transform/jobs?limit=20&status_filter=COMPLETED
```

### Health & Documentation

#### Health Check

```bash
GET /health
```

#### API Documentation

```bash
GET /docs        # Interactive Swagger UI
GET /redoc       # ReDoc documentation
```

## üèõÔ∏è Modular Architecture

### Layer Separation

```mermaid
graph TB
    subgraph "Presentation Layer"
        ROUTER[FastAPI Routers<br/>‚Ä¢ Request/Response handling<br/>‚Ä¢ Input validation<br/>‚Ä¢ HTTP status codes]
    end

    subgraph "Business Logic Layer"
        SERVICE[Service Classes<br/>‚Ä¢ Business logic<br/>‚Ä¢ Orchestration<br/>‚Ä¢ Data validation]
    end

    subgraph "Integration Layer"
        CLIENT[Client Classes<br/>‚Ä¢ External API calls<br/>‚Ä¢ Kubernetes operations<br/>‚Ä¢ Protocol handling]
    end

    subgraph "External Systems"
        EXT[External Services<br/>‚Ä¢ Airbyte<br/>‚Ä¢ Kubernetes<br/>‚Ä¢ S3 Storage]
    end

    ROUTER --> SERVICE
    SERVICE --> CLIENT
    CLIENT --> EXT
```

### Component Structure

```
app/
‚îú‚îÄ‚îÄ main.py                      # FastAPI application entry point
‚îú‚îÄ‚îÄ config.py                    # Configuration management
‚îú‚îÄ‚îÄ airbyte/                     # Airbyte integration module
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ router.py               # REST endpoints
‚îÇ   ‚îú‚îÄ‚îÄ client.py               # Airbyte API client
‚îÇ   ‚îî‚îÄ‚îÄ schemas.py              # Request/response models
‚îî‚îÄ‚îÄ data_transformation/         # Spark transformation module
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ router.py               # REST endpoints (clean, 75 lines)
    ‚îú‚îÄ‚îÄ service.py              # Business logic layer
    ‚îú‚îÄ‚îÄ client.py               # Kubernetes client layer
    ‚îî‚îÄ‚îÄ schemas.py              # Request/response models
```

### Design Principles

1. **Separation of Concerns**: Each layer has a single responsibility
2. **Dependency Injection**: Clean dependency management with FastAPI
3. **Abstraction**: External systems hidden behind client interfaces
4. **Testability**: Each component can be unit tested independently
5. **Maintainability**: Clear structure for easy maintenance and updates

## üìä Data Processing Patterns

### Medallion Architecture

The platform implements the medallion (Bronze-Silver-Gold) architecture:

```mermaid
flowchart LR
    subgraph "Bronze Layer"
        RAW[Raw Data<br/>‚Ä¢ Original format<br/>‚Ä¢ No transformations<br/>‚Ä¢ Historical archive]
    end

    subgraph "Silver Layer"
        CLEAN[Cleaned Data<br/>‚Ä¢ Data quality checks<br/>‚Ä¢ Schema standardization<br/>‚Ä¢ Deduplication]
    end

    subgraph "Gold Layer"
        AGG[Aggregated Data<br/>‚Ä¢ Business metrics<br/>‚Ä¢ Analytics ready<br/>‚Ä¢ Optimized for queries]
    end

    RAW --> CLEAN
    CLEAN --> AGG
    CLEAN --> RAW
    AGG --> RAW
```

### Transformation Workflow

1. **Bronze ‚Üí Silver**: Data cleaning, validation, standardization
2. **Silver ‚Üí Gold**: Business aggregations, metrics calculation
3. **Automated Detection**: Platform automatically detects S3 sources from Airbyte
4. **Flexible SQL**: Support for complex transformations using SQL

## üîß Configuration

### Environment Variables

| Variable                | Description                          | Default                               | Required |
| ----------------------- | ------------------------------------ | ------------------------------------- | -------- |
| `ENVIRONMENT`           | Deployment environment               | `development`                         | No       |
| `AIRBYTE_BASE_URL`      | Airbyte API base URL                 | `http://localhost:8001/api/public/v1` | Yes      |
| `SPARK_IMAGE`           | Docker image for Spark jobs          | -                                     | Yes      |
| `SPARK_SERVICE_ACCOUNT` | Kubernetes service account           | `spark-sa`                            | No       |
| `S3_SECRET_NAME`        | Kubernetes secret for S3 credentials | `s3-credentials`                      | No       |

### Request Schemas

#### Transformation Request

```json
{
  "sql": "string", // Required: SQL transformation query
  "write_mode": "overwrite|append", // Optional: Default "overwrite"
  "executor_instances": 2, // Optional: Default 2
  "executor_cores": 1, // Optional: Default 1
  "executor_memory": "512m", // Optional: Default "512m"
  "driver_cores": 1, // Optional: Default 1
  "driver_memory": "512m" // Optional: Default "512m"
}
```

#### Data Source Request

```json
{
  "source_type": "postgres|mysql|mongodb|kafka",
  "workspace_name": "string",
  "source_config": {
    "host": "string",
    "port": 5432,
    "username": "string",
    "password": "string",
    "database": "string"
  },
  "name": "string"
}
```

## üõ°Ô∏è Security & Best Practices

### Security Features

- Environment-based configuration (no secrets in code)
- SQL injection protection via parameterized queries
- Kubernetes RBAC integration
- S3 credentials managed via Kubernetes secrets

### Best Practices

- **Resource Management**: Configurable Spark resource allocation
- **Error Handling**: Comprehensive error responses and logging
- **Monitoring**: Built-in job status and metrics tracking
- **Scalability**: Horizontal scaling via Kubernetes
- **Observability**: Structured logging and metrics

## üìà Monitoring & Observability

### Job Monitoring

- Real-time job status tracking
- Driver pod logs access
- Kubernetes events monitoring
- Resource usage metrics
- Job execution history

### Operational Metrics

- Job success/failure rates
- Execution time tracking
- Resource utilization
- Queue depth monitoring

## üöÄ Deployment

The platform is designed for cloud-native deployment:

### Kubernetes Deployment

- SparkOperator for Spark job management
- Service accounts and RBAC configuration
- Resource quotas and limits
- Horizontal pod autoscaling

### Docker Support

- Multi-stage builds for optimization
- Custom Spark images with dependencies
- Environment-specific configurations

## üìã API Examples

### Complete Workflow Example

```bash
# 1. Create S3 sink for bronze data
curl -X POST "http://localhost:8000/sink" \
  -H "Content-Type: application/json" \
  -d '{
    "destination_type": "s3",
    "workspace_name": "default",
    "destination_config": {
      "s3_bucket_name": "data-lake",
      "s3_bucket_path": "bronze/"
    },
    "name": "Bronze Layer Sink"
  }'

# 2. Submit transformation job (automatically uses bronze as source)
curl -X POST "http://localhost:8000/transform" \
  -H "Content-Type: application/json" \
  -d '{
    "sql": "SELECT customer_id, SUM(amount) as total_spent, COUNT(*) as order_count FROM orders GROUP BY customer_id"
  }'

# Response: {"run_id": "abc123", "status": "submitted", ...}

# 3. Monitor job progress
curl "http://localhost:8000/transform/abc123/status"
curl "http://localhost:8000/transform/abc123/logs"

# 4. List all transformation jobs
curl "http://localhost:8000/transform/jobs"
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## üìÑ License

[Add your license information here]

The API will be available at:

- **API**: http://localhost:8001
- **API Docs**: http://localhost:8001/docs
- **Transformation**: http://localhost:8001/transformation

## üìñ API Usage

### Spark Transformations

#### Submit a Spark Transformation Job

**Note**: The `/transform` API automatically detects source and destination from Airbyte S3 sinks. All Spark configuration has sensible defaults.

```bash
curl -X POST "http://localhost:8001/transform" \
     -H "Content-Type: application/json" \
     -d '{
       "sql": "SELECT customer_id, SUM(amount) as total_amount FROM source_data GROUP BY customer_id"
     }'
```

**Optional**: You can still override Spark configuration if needed:

```bash
curl -X POST "http://localhost:8001/transform" \
     -H "Content-Type: application/json" \
     -d '{

       "sql": "SELECT customer_id, SUM(amount) as total_amount FROM source_data GROUP BY customer_id",
       "write_mode": "overwrite",
       "executor_instances": 4,
       "executor_memory": "8g"
     }'
```

### Airbyte Data Integration

#### Create a Data Source

```bash
curl -X POST "http://localhost:8001/datasource" \
     -H "Content-Type: application/json" \
     -d '{
       "source_type": "postgres",
       "workspace_name": "default",
       "source_config": {
         "host": "localhost",
         "port": 5432,
         "username": "user",
         "password": "password",
         "database": "mydb"
       },
       "name": "My Postgres Source"
     }'
```

#### Create a Data Sink

```bash
curl -X POST "http://localhost:8001/sink" \
     -H "Content-Type: application/json" \
     -d '{
       "destination_type": "s3",
       "workspace_name": "default",
       "destination_config": {
         "bucket_name": "my-data-bucket",
         "aws_access_key_id": "your_key",
         "aws_secret_access_key": "your_secret"
       },
       "name": "My S3 Sink"
     }'
```

#### Start Data Ingestion

```bash
curl -X POST "http://localhost:8001/ingestion" \
     -H "Content-Type: application/json" \
     -d '{
       "source_id": "source-uuid-here",
       "destination_id": "destination-uuid-here",
       "workspace_name": "default",
       "connection_name": "Postgres to S3 Sync"
     }'
```

#### List Data Sources and Sinks

```bash
# List data sources
curl "http://localhost:8001/datasource"

# List data sinks
curl "http://localhost:8001/sink"
```

## üìã API Endpoints

### Transformation API (Airflow Integration)

- `GET /health` - Health check

### Spark Transformation API (Kubernetes Integration)

- `POST /transform` - Submit Spark transformation job (auto-detects source/destination)

### Airbyte Integration API

- `GET /datasource` - List available data sources
- `POST /datasource` - Create new data source
- `GET /sink` - List available data sinks
- `POST /sink` - Create new data sink
- `POST /ingestion` - Start data ingestion job

### Documentation

- `GET /docs` - Interactive API documentation

## üîß Configuration

### Environment Variables

| Variable                | Description                          | Default                 |
| ----------------------- | ------------------------------------ | ----------------------- |
| `AIRBYTE_BASE_URL`      | Airbyte base URL                     | `http://localhost:8000` |
| `PIPELINE_NAMESPACE`    | Kubernetes namespace for Spark jobs  | `asgard`                |
| `SPARK_IMAGE`           | Docker image for Spark jobs          | Required                |
| `SPARK_SERVICE_ACCOUNT` | Kubernetes service account           | `spark-sa`              |
| `S3_SECRET_NAME`        | Kubernetes secret for S3 credentials | `s3-credentials`        |

### Request Schema

#### Spark Transformation Request (Kubernetes)

**Minimal Request** (recommended):

```json
{
  "sql": "string"
}
```

**With Optional Spark Configuration** (all have sensible defaults):

```json
{
  "sql": "string",
  "write_mode": "overwrite|append",
  "executor_instances": 2,
  "executor_cores": 2,
  "executor_memory": "4g",
  "driver_cores": 1,
  "driver_memory": "2g"
}
```

**Note**: Both transformation APIs automatically determine source and destination from registered Airbyte S3 sinks. The system uses the first available S3 sink as the source location and creates the destination in the same bucket under the 'silver/' folder.

#### Data Source/Sink Request

```json
{
  "source_type": "postgres|mysql|mongodb|kafka",
  "destination_type": "s3",
  "workspace_name": "default",
  "config": {
    "s3_bucket_name": "string",
    "s3_bucket_path": "string"
  },
  "name": "string"
}
```

## üèóÔ∏è Architecture

### Data Transformation Pipeline

```
Client Request ‚Üí FastAPI ‚Üí Kubernetes ‚Üí SparkApplication ‚Üí S3 Output
```

### Data Ingestion Pipeline

```
Client Request ‚Üí FastAPI ‚Üí Airbyte API ‚Üí Data Sources ‚Üí Data Sinks
```

### Unified Workflow

```
1. Data Sources ‚Üí Airbyte Ingestion ‚Üí S3 Bronze Layer
2. S3 Bronze Layer ‚Üí Spark Transformation ‚Üí S3 Silver Layer
3. S3 Silver Layer ‚Üí Further Processing ‚Üí S3 Gold Layer
```

The platform provides:

#### Airbyte Integration:

1. Simplified data source configuration (Postgres, MySQL, MongoDB, Kafka)
2. Data sink management (S3, etc.)
3. Connection and ingestion job management
4. Automated data pipeline setup

#### Airflow Integration:

1. **Automatic Source Detection**: Uses registered Airbyte S3 sinks as transformation sources

#### Spark Integration:

1. **Automatic Source Detection**: Uses registered Airbyte S3 sinks as transformation sources
2. **Medallion Architecture**: Automatically creates 'silver' layer destinations
3. Converts SQL queries to Kubernetes SparkApplications
4. Executes Spark jobs on Kubernetes using SparkOperator
5. Returns job status and execution details

#### Typical Workflow:

1. **Bronze Layer**: Register S3 sink via `/sink` endpoint ‚Üí Ingest raw data via Airbyte
2. **Silver Layer**: Submit transformation job via `/transform` ‚Üí Process data with Spark
3. **Gold Layer**: Run additional transformations for analytics-ready data

## üìÅ Project Structure

```
asgard-dev/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ airbyte/          # Airbyte integration (data ingestion)
‚îÇ   ‚îú‚îÄ‚îÄ data_transformation/ # Spark transformation (Kubernetes)
‚îÇ   ‚îú‚îÄ‚îÄ config.py         # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ main.py           # FastAPI app
‚îú‚îÄ‚îÄ .env                  # Environment variables
‚îú‚îÄ‚îÄ pyproject.toml        # Dependencies and config
‚îú‚îÄ‚îÄ uv.lock               # Lock file
```

## üîí Security

- Basic SQL injection protection
- Environment-based configuration
- No sensitive data in code
