# Multi-stage Dockerfile for ML Model Inference Service
# Optimized for production deployment with MLflow and Feast support

# ============================================================================
# Stage 1: Builder (Install dependencies)
# ============================================================================
FROM python:3.11-slim as builder

WORKDIR /build

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY ml_deployment/requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# ============================================================================
# Stage 2: Runtime (Production image)
# ============================================================================
FROM python:3.11-slim

LABEL maintainer="asgard-platform"
LABEL description="ML Model Inference Service with MLflow and Feast"

# Set environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PORT=8080

# Create non-root user
RUN groupadd -r mluser && useradd -r -g mluser mluser

# Set working directory
WORKDIR /app

# Copy Python packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# Copy application code
COPY ml_deployment/inference_service.py .

# Create necessary directories
RUN mkdir -p /tmp/feast_repo /tmp/models && \
    chown -R mluser:mluser /app /tmp/feast_repo /tmp/models

# Switch to non-root user
USER mluser

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8080/health').raise_for_status()" || exit 1

# Run the application
CMD ["python", "inference_service.py"]
