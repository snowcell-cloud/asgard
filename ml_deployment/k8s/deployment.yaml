---
# Namespace for ML inference service
apiVersion: v1
kind: Namespace
metadata:
  name: ml-inference
  labels:
    name: ml-inference
    environment: production

---
# ConfigMap for inference service configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-inference-config
  namespace: ml-inference
data:
  MLFLOW_TRACKING_URI: "http://mlflow-service.asgard.svc.cluster.local:5000"
  MODEL_NAME: "churn_predictor_feast"
  MODEL_VERSION: "latest"
  PORT: "8080"
  FEAST_REPO_PATH: "/tmp/feast_repo"
  AWS_REGION: "eu-north-1"
  S3_BUCKET: "airbytedestination1"

---
# Secret for AWS credentials (create this manually)
# kubectl create secret generic aws-credentials \
#   --from-literal=AWS_ACCESS_KEY_ID=<your-key> \
#   --from-literal=AWS_SECRET_ACCESS_KEY=<your-secret> \
#   -n ml-inference
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
  namespace: ml-inference
type: Opaque
# data:
#   AWS_ACCESS_KEY_ID: <base64-encoded>
#   AWS_SECRET_ACCESS_KEY: <base64-encoded>

---
# Deployment for ML inference service
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-inference
  namespace: ml-inference
  labels:
    app: ml-inference
    version: v1
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ml-inference
  template:
    metadata:
      labels:
        app: ml-inference
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: ml-inference
      containers:
        - name: inference
          image: <AWS_ACCOUNT_ID>.dkr.ecr.<REGION>.amazonaws.com/ml-inference:latest
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: MLFLOW_TRACKING_URI
              valueFrom:
                configMapKeyRef:
                  name: ml-inference-config
                  key: MLFLOW_TRACKING_URI
            - name: MODEL_NAME
              valueFrom:
                configMapKeyRef:
                  name: ml-inference-config
                  key: MODEL_NAME
            - name: MODEL_VERSION
              valueFrom:
                configMapKeyRef:
                  name: ml-inference-config
                  key: MODEL_VERSION
            - name: PORT
              valueFrom:
                configMapKeyRef:
                  name: ml-inference-config
                  key: PORT
            - name: AWS_REGION
              valueFrom:
                configMapKeyRef:
                  name: ml-inference-config
                  key: AWS_REGION
            - name: S3_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: ml-inference-config
                  key: S3_BUCKET
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: AWS_ACCESS_KEY_ID
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-credentials
                  key: AWS_SECRET_ACCESS_KEY
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "2Gi"
              cpu: "1000m"
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3
          volumeMounts:
            - name: tmp
              mountPath: /tmp
      volumes:
        - name: tmp
          emptyDir: {}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

---
# ServiceAccount for ML inference service
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ml-inference
  namespace: ml-inference
  annotations:
    # If using IRSA (IAM Roles for Service Accounts)
    # eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/ml-inference-role

---
# Service for ML inference
apiVersion: v1
kind: Service
metadata:
  name: ml-inference-service
  namespace: ml-inference
  labels:
    app: ml-inference
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
      name: http
  selector:
    app: ml-inference

---
# Ingress for external access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ml-inference-ingress
  namespace: ml-inference
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  tls:
    - hosts:
        - ml-inference.yourdomain.com
      secretName: ml-inference-tls
  rules:
    - host: ml-inference.yourdomain.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: ml-inference-service
                port:
                  number: 80

---
# HorizontalPodAutoscaler for auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-inference-hpa
  namespace: ml-inference
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-inference
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 2
          periodSeconds: 30
      selectPolicy: Max

---
# PodDisruptionBudget for high availability
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ml-inference-pdb
  namespace: ml-inference
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: ml-inference
