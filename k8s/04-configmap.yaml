# ConfigMap containing the SQL transformation script
apiVersion: v1
kind: ConfigMap
metadata:
  name: sql-transform-script
  namespace: asgard
  labels:
    app: spark
data:
  sql_transform.py: |
    #!/usr/bin/env python3
    """
    SQL transformation script for Spark on Kubernetes.
    Reads from S3 parquet files and applies SQL transformations.
    """

    import os
    import sys
    import json
    from pyspark.sql import SparkSession

    def main():
        print("🚀 Starting SQL transformation...")
        
        # Get configuration from environment variables
        sql_query = os.getenv("SQL_QUERY")
        source_paths_json = os.getenv("SOURCE_PATHS")
        destination_path = os.getenv("DESTINATION_PATH")
        write_mode = os.getenv("WRITE_MODE", "overwrite")
        
        # Validate required environment variables
        if not sql_query:
            print("❌ ERROR: SQL_QUERY environment variable is required")
            sys.exit(1)
        
        if not source_paths_json:
            print("❌ ERROR: SOURCE_PATHS environment variable is required")
            print("    This should be set by the transformation API")
            sys.exit(1)
        
        if not destination_path:
            print("❌ ERROR: DESTINATION_PATH environment variable is required") 
            print("    This should be set by the transformation API")
            sys.exit(1)
        
        print(f"SQL Query: {sql_query}")
        print(f"Source paths: {source_paths_json}")
        print(f"Destination: {destination_path}")
        print(f"Write mode: {write_mode}")
        
        # Parse source paths
        try:
            source_paths = json.loads(source_paths_json)
        except json.JSONDecodeError as e:
            print(f"❌ Error parsing source paths: {e}")
            sys.exit(1)
        
        # Initialize Spark session with S3 configuration
        print("🔧 Initializing Spark session...")
        spark = SparkSession.builder \
            .appName("SQL Data Transformation") \
            .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
            .config("spark.hadoop.fs.s3a.access.key", os.getenv("AWS_ACCESS_KEY_ID")) \
            .config("spark.hadoop.fs.s3a.secret.key", os.getenv("AWS_SECRET_ACCESS_KEY")) \
            .config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .config("spark.hadoop.fs.s3a.fast.upload", "true") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()
        
        print("✅ Spark session initialized successfully")
        
        try:
            # Read data from all source paths
            print(f"📖 Reading data from {len(source_paths)} source(s)...")
            dfs = []
            
            for source_path in source_paths:
                print(f"Reading from: {source_path}")
                try:
                    df = spark.read.parquet(source_path)
                    row_count = df.count()
                    print(f"✅ Successfully read {row_count} rows from {source_path}")
                    if row_count > 0:
                        dfs.append(df)
                    else:
                        print(f"⚠️  Warning: No data found in {source_path}")
                except Exception as e:
                    print(f"❌ Error reading from {source_path}: {e}")
            
            if not dfs:
                print("❌ No data found in any source paths")
                sys.exit(1)
            
            # Union all dataframes
            print("🔄 Combining data from all sources...")
            if len(dfs) == 1:
                combined_df = dfs[0]
            else:
                combined_df = dfs[0]
                for df in dfs[1:]:
                    combined_df = combined_df.union(df)
            
            total_rows = combined_df.count()
            print(f"📊 Combined dataset has {total_rows} total rows")
            
            # Show schema and sample data
            print("📋 Schema:")
            combined_df.printSchema()
            
            print("📄 Sample data (first 5 rows):")
            combined_df.show(5, truncate=False)
            
            # Create temporary view for SQL
            combined_df.createOrReplaceTempView("source_data")
            print("🔍 Created temporary view 'source_data'")
            
            # Execute SQL transformation
            print(f"⚡ Executing SQL transformation: {sql_query}")
            result_df = spark.sql(sql_query)
            
            result_count = result_df.count()
            print(f"✅ SQL transformation completed. Result has {result_count} rows")
            
            print("📄 Transformation result sample (first 5 rows):")
            result_df.show(5, truncate=False)
            
            # Write results to destination
            print(f"💾 Writing {result_count} rows to: {destination_path}")
            result_df.coalesce(1) \
                .write \
                .mode(write_mode) \
                .parquet(destination_path)
            
            print("🎉 ✅ Transformation completed successfully!")
            
        except Exception as e:
            print(f"❌ Error during transformation: {str(e)}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
            
        finally:
            print("🛑 Stopping Spark session...")
            spark.stop()

    if __name__ == "__main__":
        main()
